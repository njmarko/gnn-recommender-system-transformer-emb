{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code in .ipynb file for testing on AI platrofm\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from data_load_movies import load_movies\n",
    "from data_load_ratings import load_ratings\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import HeteroData\n",
    "from torch_geometric.loader import DataLoader, LinkNeighborLoader\n",
    "from torch_geometric.nn.conv.gcn_conv import gcn_norm\n",
    "from torch_geometric.transforms import RandomLinkSplit, ToUndirected\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "from model.bipartite_sage import MetaSage\n",
    "from model.bipartite_gat import MetaGATv2\n",
    "from model.model import Model\n",
    "from model.graph_transformer import MetaTransformerGat\n",
    "\n",
    "def load_data(args):\n",
    "    movies, movie_mappings = load_movies(args.word_embeddings)\n",
    "\n",
    "\n",
    "    ratings, user_mappings = load_ratings()\n",
    "\n",
    "\n",
    "    src = [user_mappings[index] for index in ratings.index]\n",
    "    dst = [movie_mappings[index] for index in ratings['movieId']]\n",
    "    edge_index = torch.tensor([src, dst])\n",
    "    edge_attrs = [\n",
    "        torch.tensor(ratings[column].values).unsqueeze(dim=1) for column in ['rating', 'timestamp'] \n",
    "    ]\n",
    "    edge_label = torch.cat(edge_attrs, dim=-1)\n",
    "\n",
    "    # movies = movies.fillna(0).astype(np.float32)\n",
    "    products_tensor = torch.from_numpy(movies.values).to(torch.float32) # TODO: PCA\n",
    "    # customer_tensor = torch.from_numpy(ratings.index.values).to(torch.float32).unsqueeze(dim=-1)\n",
    "    # customer_tensor = torch.arange(len(user_mappings)).to(torch.float32).unsqueeze(dim=-1)\n",
    "    # customer_tensor = torch.rand(len(user_mappings), 1)\n",
    "    customer_tensor = torch.arange(len(user_mappings))\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "    Feature Engineering: If user IDs are all you have, consider engineering synthetic features. This could include:\n",
    "    Node degree (number of connections each user has with various products).\n",
    "    Clustering coefficient (a measure of the degree to which nodes in a graph tend to cluster together).\n",
    "    Any available metadata from interaction patterns (e.g., frequency or recency of user-product interactions).\n",
    "    \"\"\"\n",
    "\n",
    "    # print(products_tensor.shape)\n",
    "    # print(customer_tensor.shape)\n",
    "    # print(customer_tensor)\n",
    "\n",
    "    data = HeteroData()\n",
    "    # data['customer'].num_nodes = len(user_mappings)\n",
    "    data['customer'].x = customer_tensor\n",
    "    data['product'].x = products_tensor\n",
    "\n",
    "    data['customer', 'buys', 'product'].edge_index = edge_index\n",
    "    data['customer', 'buys', 'product'].edge_label = edge_label\n",
    "\n",
    "\n",
    "    data = ToUndirected()(data)\n",
    "\n",
    "    train_data, val_data, test_data = RandomLinkSplit(\n",
    "        num_val=args.val_split,\n",
    "        num_test=args.test_split,\n",
    "        is_undirected=True,\n",
    "        neg_sampling_ratio=0.0,\n",
    "        edge_types=[('customer', 'buys', 'product')],\n",
    "        rev_edge_types=[('product', 'rev_buys', 'customer')],\n",
    "    )(data)\n",
    "\n",
    "    if args.model in [\"meta_sage\", \"meta_gatv2\"]:\n",
    "        # Generate the co-occurence matrix of movies<>movies:\n",
    "        metapath = [('product', 'rev_buys', 'customer'), ('customer', 'buys', 'product')]\n",
    "        train_data = T.AddMetaPaths(metapaths=[metapath])(train_data)\n",
    "\n",
    "        # Apply normalization to filter the metapath:\n",
    "        _, edge_weight = gcn_norm(\n",
    "            train_data['product', 'product'].edge_index,\n",
    "            num_nodes=train_data['product'].num_nodes,\n",
    "            add_self_loops=False,\n",
    "        )\n",
    "        edge_index = train_data['product', 'product'].edge_index[:, edge_weight > 0.002]\n",
    "        # train_data['product', 'metapath_0', 'product'].edge_index = edge_index\n",
    "\n",
    "        # TODO: Metapaths for customers. Doesnt make much sense if there are no features for customers\n",
    "\n",
    "        train_data['product', 'metapath_0', 'product'].edge_index = edge_index\n",
    "        val_data['product', 'metapath_0', 'product'].edge_index = edge_index\n",
    "        test_data['product', 'metapath_0', 'product'].edge_index = edge_index\n",
    "\n",
    "    # print(train_data)\n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "def split_data(data, val_ratio=0.15, test_ratio=0.15):\n",
    "    transform = RandomLinkSplit(\n",
    "        num_val=val_ratio,\n",
    "        num_test=test_ratio,\n",
    "        is_undirected=True,\n",
    "        neg_sampling_ratio=0.0,\n",
    "        edge_types=[('customer', 'buys', 'product')],\n",
    "        rev_edge_types=[('product', 'rev_buys', 'customer')],\n",
    "    )\n",
    "    return transform(data)\n",
    "\n",
    "def weighted_mse_loss(pred, target, weight=None):\n",
    "    weight = torch.tensor([1.]) if weight is None else weight[target.to('cpu').long()].to(pred.dtype)\n",
    "    # diff = pred - target.to(pred.dtype)\n",
    "    # weighted_diff = weight * diff.pow(2)\n",
    "    # sum_loss = weighted_diff\n",
    "    # loss = sum_loss.mean()\n",
    "    loss = (weight.to(pred.device) * (pred - target.to(pred.dtype)).pow(2)).mean()\n",
    "    return loss\n",
    "\n",
    "def train(model, data_loader, optimizer, weight=None, scheduler=None, args=None, wandb=None):\n",
    "    model.train()\n",
    "    total_loss = total_nodes = 0\n",
    "    i = 0\n",
    "    for data in tqdm(data_loader):\n",
    "        data.to(args.device)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(data.x_dict, data.edge_index_dict,\n",
    "                     data['customer', 'product'].edge_label_index,\n",
    "                     edge_label=data['product', 'rev_buys', 'customer'].edge_label[:,1:]\n",
    "                     ).squeeze(axis=-1)\n",
    "        target = data['customer', 'product'].edge_label[:,0]\n",
    "        # loss = weighted_mse_loss(pred, target, weight) # TODO: Add some other loss\n",
    "        loss = F.mse_loss(pred, target.to(pred.dtype))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "        if i % 5 == 0 and scheduler:\n",
    "            wandb.log({\"train_lr\": scheduler.get_last_lr()[0]},\n",
    "                      # commit=False, # Commit=False just accumulates data\n",
    "                      )\n",
    "\n",
    "        total_loss += loss.item() * pred.numel()\n",
    "        total_nodes += pred.numel()\n",
    "\n",
    "    return float(total_loss/total_nodes)\n",
    "\n",
    "import torch  # Ensure torch is imported\n",
    "\n",
    "@torch.no_grad()\n",
    "def old_test(model, data):\n",
    "    model.eval()\n",
    "    pred = model(data.x_dict, data.edge_index_dict,\n",
    "                 data['customer', 'product'].edge_label_index,\n",
    "                 edge_label=data['product', 'rev_buys', 'customer'].edge_label[:,1:]\n",
    "                 ).squeeze(axis=-1)\n",
    "    pred = pred.clamp(min=0, max=5)\n",
    "    target = data['customer', 'product'].edge_label[:,0].float()\n",
    "    rmse = F.mse_loss(pred, target).sqrt()\n",
    "    return float(rmse)\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, loader):\n",
    "    model.eval()  # Ensure the model is in evaluation mode\n",
    "    total_sq_error = 0  # Total squared error\n",
    "    total_samples = 0   # Total number of samples\n",
    "\n",
    "    for data in loader:\n",
    "        pred = model(data.x_dict, data.edge_index_dict,\n",
    "                     data['customer', 'product'].edge_label_index,\n",
    "                     edge_label=data['product', 'rev_buys', 'customer'].edge_label[:,1:]\n",
    "                     ).squeeze(-1)\n",
    "        pred = pred.clamp(min=0, max=5)  # Clamping predictions\n",
    "        target = data['customer', 'product'].edge_label[:,0].float()\n",
    "        \n",
    "        # Calculate squared error and accumulate\n",
    "        target = target.to(pred.device)\n",
    "        sq_error = F.mse_loss(pred, target, reduction='sum')\n",
    "        total_sq_error += sq_error.item()\n",
    "        total_samples += data.num_nodes  # Update the count of total samples\n",
    "\n",
    "    # Calculate the average of the squared errors, then take the square root using torch.sqrt\n",
    "    average_rmse = torch.sqrt(torch.tensor(total_sq_error / total_samples))\n",
    "    return average_rmse\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# @torch.no_grad()\n",
    "# def test(model, data):\n",
    "#     pred = model(data.x_dict, data.edge_index_dict,\n",
    "#                  data['customer', 'product'].edge_label_index,\n",
    "#                  edge_label=data['product', 'rev_buys', 'customer'].edge_label[:,1:]\n",
    "#                  ).squeeze(axis=-1)\n",
    "#     pred = pred.clamp(min=0, max=5)\n",
    "#     target = data['customer', 'product'].edge_label[:,0].float()\n",
    "#     rmse = F.mse_loss(pred, target).sqrt()\n",
    "#     return float(rmse)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def top_at_k(model, src, dst, train_data, test_data, k=10):\n",
    "    customer_idx = random.randint(0, len(src) - 1)\n",
    "    customer_row = torch.tensor([customer_idx] * len(dst))\n",
    "    all_product_ids = torch.arange(len(dst))\n",
    "    edge_label_index = torch.stack([customer_row, all_product_ids], dim=0)\n",
    "    pred = model(train_data.x_dict, train_data.edge_index_dict,\n",
    "                 edge_label_index)\n",
    "    pred = pred.clamp(min=0, max=5)\n",
    "\n",
    "    # we will only select movies for the user where the predicting rating is =5\n",
    "    rec_product_ids = (pred[:, 0] == 5).nonzero(as_tuple=True)\n",
    "    top_k_recommendations = [rec_product for rec_product in rec_product_ids[0].tolist()[:k]]\n",
    "\n",
    "    test_edge_label_index = test_data['customer', 'product'].edge_label_index\n",
    "    customer_interacted_products = test_edge_label_index[1, test_edge_label_index[0] == customer_idx]\n",
    "\n",
    "    hits = 0\n",
    "    for product_idx in top_k_recommendations:\n",
    "        if product_idx in customer_interacted_products: hits += 1\n",
    "\n",
    "    return hits / k\n",
    "\n",
    "def find_first_component(variance_ratio, theshold=0.9):\n",
    "    cumulative_variance = np.cumsum(variance_ratio)\n",
    "    for i, ratio in enumerate(cumulative_variance):\n",
    "        if ratio > theshold:\n",
    "            return i + 1\n",
    "    return -1\n",
    "\n",
    "def plot_variance_ratio(data, threshold=0.9, plot=True):\n",
    "    num_features = data.shape[1]\n",
    "    pca = PCA(n_components=num_features)\n",
    "    pca.fit(data)\n",
    "    variance_ratio = pca.explained_variance_ratio_\n",
    "    num_components = range(1, num_features + 1)\n",
    "\n",
    "    if plot:\n",
    "        plt.plot(num_components, np.cumsum(variance_ratio))\n",
    "        for i in range(10, len(num_components), 10):\n",
    "            plt.axvline(x=i, color='gray', linestyle='--')\n",
    "            plt.text(i, np.cumsum(variance_ratio)[i-1], f'{np.cumsum(variance_ratio)[i-1]:.2f}', ha='center', va='bottom')\n",
    "        plt.xlabel('Number of Components')\n",
    "        plt.ylabel('Cumulative Explained Variance')\n",
    "        plt.title('Explained Variance vs. Number of Components')\n",
    "        plt.show()\n",
    "\n",
    "    return find_first_component(variance_ratio, threshold)\n",
    "\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    if args.track_run:\n",
    "        import wandb\n",
    "    args.device = 'cuda' if torch.cuda.is_available() and (args.device == 'cuda') else 'cpu'\n",
    "    if args.track_run:\n",
    "        wb_run_train = wandb.init(entity=args.entity, project=args.project_name, group=args.group,\n",
    "                                  # save_code=True, # Pycharm complains about duplicate code fragments\n",
    "                                  job_type=args.job_type,\n",
    "                                  tags=args.tags,\n",
    "                                  name=f'{args.model}_train_pca_{args.pca}_var_{args.variance}' if args.use_variance_threshold else f'{args.model}_train_pca_{args.pca}_components_{args.pca_components}',\n",
    "                                  config=args,\n",
    "                                  )\n",
    "    # graph_data = load_data(args)\n",
    "    # train_data, val_data, test_data = split_data(graph_data, args.val_split, args.test_split)\n",
    "    train_data, val_data, test_data = load_data(args)\n",
    "\n",
    "    train_data: HeteroData\n",
    "\n",
    "    if args.standardize_edge_features:\n",
    "        standard_scaler_edge = StandardScaler()\n",
    "\n",
    "        edge_attr = train_data['customer','buys','product'].edge_label[:,1:]\n",
    "        train_data['customer','buys','product'].edge_label[:,1:] = torch.from_numpy(standard_scaler_edge.fit_transform(edge_attr)).float()\n",
    "        edge_attr = train_data['product','rev_buys','customer'].edge_label[:,1:]\n",
    "        train_data['product', 'rev_buys', 'customer'].edge_label[:,1:] = torch.from_numpy(standard_scaler_edge.transform(edge_attr)).float()\n",
    "\n",
    "        edge_attr = val_data['customer','buys','product'].edge_label[:,1:]\n",
    "        val_data['customer','buys','product'].edge_label[:,1:] = torch.from_numpy(standard_scaler_edge.transform(edge_attr)).float()\n",
    "        edge_attr = val_data['product','rev_buys','customer'].edge_label[:,1:]\n",
    "        val_data['product', 'rev_buys', 'customer'].edge_label[:,1:] = torch.from_numpy(standard_scaler_edge.transform(edge_attr)).float()\n",
    "\n",
    "        edge_attr = test_data['customer','buys','product'].edge_label[:,1:]\n",
    "        test_data['customer','buys','product'].edge_label[:,1:] = torch.from_numpy(standard_scaler_edge.transform(edge_attr)).float()\n",
    "        edge_attr = test_data['product','rev_buys','customer'].edge_label[:,1:]\n",
    "        test_data['product', 'rev_buys', 'customer'].edge_label[:,1:] = torch.from_numpy(standard_scaler_edge.transform(edge_attr)).float()\n",
    "\n",
    "    \n",
    "    # Standardize product features\n",
    "\n",
    "    num_features = 187 \n",
    "    if args.standardize_product_features:\n",
    "        scaler = StandardScaler()\n",
    "\n",
    "        # Extract non-embedding features (first 187 columns assumed to be non-embedding features)\n",
    "        train_movie_features = train_data['product'].x[:, :num_features].numpy()\n",
    "        scaler.fit(train_movie_features)\n",
    "\n",
    "        # Transform these features in the training data\n",
    "        train_data['product'].x[:, :num_features] = torch.from_numpy(scaler.transform(train_movie_features)).float()\n",
    "\n",
    "        # Apply the same transformation to the validation and test data\n",
    "        val_movie_features = val_data['product'].x[:, :num_features].numpy()\n",
    "        val_data['product'].x[:, :num_features] = torch.from_numpy(scaler.transform(val_movie_features)).float()\n",
    "\n",
    "        test_movie_features = test_data['product'].x[:, :num_features].numpy()\n",
    "        test_data['product'].x[:, :num_features] = torch.from_numpy(scaler.transform(test_movie_features)).float()\n",
    "\n",
    "\n",
    "    # # Use only the text embeddings\n",
    "    if args.pca == \"embeddings\":\n",
    "        train_data['product'].x = train_data['product'].x[:, num_features:]\n",
    "        val_data['product'].x = val_data['product'].x[:, num_features:]\n",
    "        test_data['product'].x = test_data['product'].x[:, num_features:]\n",
    "    # Use only the original features, without the text embeddings\n",
    "    if args.pca == \"features\":\n",
    "        train_data['product'].x = train_data['product'].x[:, :num_features]\n",
    "        val_data['product'].x = val_data['product'].x[:, :num_features]\n",
    "        test_data['product'].x = test_data['product'].x[:, :num_features]\n",
    "    print(\"Number of columns in train_data['product']: \", train_data['product'].x.shape[1])\n",
    "    print(\"Number of columns in val_data['product']: \", val_data['product'].x.shape[1])\n",
    "    print(\"Number of columns in test_data['product']: \", test_data['product'].x.shape[1])\n",
    "\n",
    "\n",
    "    if args.pca in [\"features\",\"embeddings\", \"combined\"]:\n",
    "        if args.use_variance_threshold:\n",
    "            n_component_with_desired_variance = plot_variance_ratio(train_data, threshold=args.variance, plot=args.plot_variance)\n",
    "        else:\n",
    "            n_component_with_desired_variance = args.pca_components\n",
    "        print(\"Using n components: \", n_component_with_desired_variance)\n",
    "        pca = PCA(n_components=n_component_with_desired_variance)\n",
    "        pca.fit(train_data['product'].x)\n",
    "        variance_ratio = pca.explained_variance_ratio_\n",
    "        print(\"Variance ratio sum:\", variance_ratio.sum())\n",
    "\n",
    "        train_data['product'].x = torch.from_numpy(pca.transform(train_data['product'].x)).float()\n",
    "        val_data['product'].x = torch.from_numpy(pca.transform(val_data['product'].x)).float()\n",
    "        test_data['product'].x = torch.from_numpy(pca.transform(test_data['product'].x)).float()\n",
    "\n",
    "    if args.pca == \"separated\":\n",
    "        # Doing separate PCA for the text embeddings and the non-embedding features\n",
    "        if args.use_variance_threshold:\n",
    "            n_component_with_desired_variance = plot_variance_ratio(train_data['product'].x[:,:num_features], threshold=args.variance, plot=args.plot_variance)\n",
    "        else:\n",
    "            n_component_with_desired_variance = args.pca_components\n",
    "        print(\"Using n components for features: \", n_component_with_desired_variance)\n",
    "        pca_features = PCA(n_components=n_component_with_desired_variance)\n",
    "        pca_features.fit(train_data['product'].x[:,:num_features])\n",
    "        variance_ratio = pca_features.explained_variance_ratio_\n",
    "        print(\"Variance ratio sum features:\", variance_ratio.sum())\n",
    "\n",
    "        if args.use_variance_threshold:\n",
    "            n_component_with_desired_variance = plot_variance_ratio(train_data['product'].x[:,num_features:], threshold=args.variance, plot=args.plot_variance)\n",
    "        else:\n",
    "            n_component_with_desired_variance = args.pca_components\n",
    "        print(\"Using n components for embeddings: \", n_component_with_desired_variance)\n",
    "        pca_embeddings = PCA(n_components=n_component_with_desired_variance)\n",
    "        pca_embeddings.fit(train_data['product'].x[:,num_features:])\n",
    "        variance_ratio = pca_embeddings.explained_variance_ratio_\n",
    "        print(\"Variance ratio sum embeddings:\", variance_ratio.sum())\n",
    "\n",
    "        # transformed_features = pca_features.transform(train_data['product'].x[:, :num_features])\n",
    "        # print(transformed_features.shape)  # Check the shape after transformation\n",
    "        # print(train_data['product'].x[:, :num_features].shape)  # Check the original tensor's shape\n",
    "\n",
    "\n",
    "        # For training data\n",
    "        features_train = torch.from_numpy(pca_features.transform(train_data['product'].x[:, :num_features])).float()\n",
    "        embeddings_train = torch.from_numpy(pca_embeddings.transform(train_data['product'].x[:, num_features:])).float()\n",
    "        train_data['product'].x = torch.cat((features_train, embeddings_train), dim=1)\n",
    "        print(train_data['product'].x.shape)\n",
    "\n",
    "        # For validation data\n",
    "        features_val = torch.from_numpy(pca_features.transform(val_data['product'].x[:, :num_features])).float()\n",
    "        embeddings_val = torch.from_numpy(pca_embeddings.transform(val_data['product'].x[:, num_features:])).float()\n",
    "        val_data['product'].x = torch.cat((features_val, embeddings_val), dim=1)\n",
    "\n",
    "        # For test data\n",
    "        features_test = torch.from_numpy(pca_features.transform(test_data['product'].x[:, :num_features])).float()\n",
    "        embeddings_test = torch.from_numpy(pca_embeddings.transform(test_data['product'].x[:, num_features:])).float()\n",
    "        test_data['product'].x = torch.cat((features_test, embeddings_test), dim=1)\n",
    "\n",
    "    print(\"Number of columns in train_data['product']: \", train_data['product'].x.shape[1])\n",
    "    print(\"Number of columns in val_data['product']: \", val_data['product'].x.shape[1])\n",
    "    print(\"Number of columns in test_data['product']: \", test_data['product'].x.shape[1])\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    # print(train_data['product'].x)\n",
    "\n",
    "\n",
    "    # ============\n",
    "    # BATCH SETUP\n",
    "    # ===========\n",
    "    edge_label_index = train_data['customer', 'buys', 'product'].edge_label_index\n",
    "    edge_label = train_data['customer', 'buys', 'product'].edge_label\n",
    "\n",
    "    train_loader = LinkNeighborLoader(\n",
    "        train_data.to(args.device),\n",
    "        num_neighbors=[15]*3,\n",
    "        batch_size=args.batch_size,\n",
    "        edge_label_index=(('customer', 'buys', 'product'), edge_label_index),\n",
    "        edge_label=edge_label,\n",
    "        shuffle=True,\n",
    "        # num_workers=4\n",
    "    )\n",
    "\n",
    "    val_loader = LinkNeighborLoader(\n",
    "        val_data.to(args.device),\n",
    "        num_neighbors=[10]*3,  # You might choose to have fewer layers or fewer neighbors\n",
    "        batch_size=100,\n",
    "        edge_label_index=(('customer', 'buys', 'product'), val_data['customer', 'buys', 'product'].edge_label_index),\n",
    "        edge_label=val_data['customer', 'buys', 'product'].edge_label,\n",
    "        shuffle=False  # Typically, we do not shuffle validation data\n",
    "    )\n",
    "\n",
    "    test_loader = LinkNeighborLoader(\n",
    "        test_data.to(args.device),\n",
    "        num_neighbors=[10]*3,  # Adjust the number of neighbors according to the model's requirements\n",
    "        batch_size=100,  # Adjust the batch size based on GPU capacity and dataset size\n",
    "        edge_label_index=(('customer', 'buys', 'product'), test_data['customer', 'buys', 'product'].edge_label_index),\n",
    "        edge_label=test_data['customer', 'buys', 'product'].edge_label,\n",
    "        shuffle=False  # Shuffling is not necessary for test data\n",
    "    )\n",
    "\n",
    "    # We have an unbalanced dataset with many labels for rating 3 and 4, and very\n",
    "    # few for 0 and 1, therefore we use a weighted MSE loss.\n",
    "    if args.use_weighted_loss:\n",
    "        weight = torch.bincount(train_data['customer', 'product'].edge_label[:,0].long())\n",
    "        weight = weight.max() / weight\n",
    "        weight.to(args.device)\n",
    "    else:\n",
    "        weight = None\n",
    "    if args.model == 'graph_sage':\n",
    "        model = Model(hidden_channels=args.hidden_channels, out_channels=args.out_channels, edge_features=1, metadata=train_data.metadata())\n",
    "    elif args.model == 'meta_sage':\n",
    "        model = MetaSage(train_data['customer'].num_nodes, hidden_channels=args.hidden_channels, out_channels=args.out_channels)\n",
    "    elif args.model == 'meta_gatv2':\n",
    "        model = MetaGATv2(train_data['customer'].num_nodes, hidden_channels=args.hidden_channels, out_channels=args.out_channels, edge_channels=args.edge_channels)\n",
    "    elif args.model == 'graph_transformer':\n",
    "        model = MetaTransformerGat(train_data['customer'].num_nodes, hidden_channels=args.hidden_channels, out_channels=args.out_channels)\n",
    "    model.to(args.device)\n",
    "\n",
    "    # Due to lazy initialization, we need to run one model step so the number\n",
    "    # of parameters can be inferred:\n",
    "    # with torch.no_grad():\n",
    "        # if args.model == 'graph_sage':\n",
    "            # model.encoder(train_data.x_dict.to(args.device), train_data.edge_index_dict.to(args.device))\n",
    "\n",
    "    # ========================\n",
    "    # OPTIMIZER AND SETUP DATA\n",
    "    # ========================\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "        # Scheduler options\n",
    "    # parser.add_argument('-sch', '--scheduler', type=str.lower, default='cycliclr',\n",
    "    #                     choices=scheduler_choices.keys(),\n",
    "    #                     help=f'Optimizer to be used {scheduler_choices.keys()}')\n",
    "    # parser.add_argument('-base_lr', '--base_lr', type=float, default=3e-4,\n",
    "    #                     help=\"Base learning rate for scheduler\")\n",
    "    # parser.add_argument('-max_lr', '--max_lr', type=float, default=0.001,\n",
    "    #                     help=\"Max learning rate for scheduler\")\n",
    "    # parser.add_argument('-step_size_up', '--step_size_up', type=int, default=0,\n",
    "    #                     help=\"CycleLR scheduler: step size up. If 0, then it is automatically calculated.\")\n",
    "    # parser.add_argument('-cyc_mom', '--cycle_momentum', type=bool, default=False,\n",
    "    #                     help=\"CyclicLR scheduler: cycle momentum in scheduler\")\n",
    "    # parser.add_argument('-sch_m', '--scheduler_mode', type=str, default=\"triangular2\",\n",
    "    #                     choices=['triangular', 'triangular2', 'exp_range'],\n",
    "    #                     help=f\"CyclicLR scheduler: mode {['triangular', 'triangular2', 'exp_range']}\")\n",
    "    if args.step_size_up <= 0:\n",
    "        args.step_size_up = len(train_loader.dataset) // args.batch_size\n",
    "    print(args.step_size_up)\n",
    "    scheduler = torch.optim.lr_scheduler.CyclicLR(\n",
    "        optimizer=optimizer,\n",
    "        base_lr=args.base_lr,\n",
    "        max_lr=args.max_lr,\n",
    "        step_size_up=args.step_size_up,\n",
    "        mode=args.scheduler_mode,\n",
    "        cycle_momentum=False,\n",
    "        # gamma=0.9, \n",
    "    )\n",
    "\n",
    "    best_model_loss = np.Inf\n",
    "    best_model_path = None\n",
    "    for epoch in range(0, args.no_epochs):\n",
    "        loss = train(model, train_loader, optimizer, weight, scheduler, args, wandb)\n",
    "        # train_rmse = test(model, train_loader)\n",
    "        # val_rmse = test(model, val_loader)\n",
    "        val_rmse = old_test(model, val_data.to(args.device))\n",
    "        if args.track_run:\n",
    "            wb_run_train.log({'train_epoch_loss': loss, \n",
    "                            #   'train_epoch_rmse': train_rmse,\n",
    "                              'val_epoch_rmse': val_rmse})\n",
    "        print(f'Epoch: {epoch + 1:03d}, Loss: {loss:.4f}, '\n",
    "            #   f'Train: {train_rmse:.4f}, '\n",
    "              f'Val: {val_rmse:.4f}')\n",
    "        if val_rmse < best_model_loss:\n",
    "            best_model_loss = val_rmse\n",
    "            Path(f'../experiments/{args.group}').mkdir(exist_ok=True, parents=True)\n",
    "            new_best_path = os.path.join(f'../experiments/{args.group}',\n",
    "                                         f'train-{args.group}-{args.model}-epoch{epoch + 1}'\n",
    "                                         f'-loss{val_rmse:.4f}.pt')\n",
    "            torch.save(model.state_dict(), new_best_path)\n",
    "            if best_model_path:\n",
    "                os.remove(best_model_path)\n",
    "            best_model_path = new_best_path\n",
    "    if args.track_run:\n",
    "        wb_run_train.finish()\n",
    "\n",
    "    args.job_type = \"eval\"\n",
    "    if args.track_run:\n",
    "        wb_run_eval = wandb.init(entity=args.entity, project=args.project_name, group=args.group,\n",
    "                                 # save_code=True, # Pycharm complains about duplicate code fragments\n",
    "                                 job_type=args.job_type,\n",
    "                                 tags=args.tags,\n",
    "                                 name=f'{args.model}_eval_pca_{args.pca}_var_{args.variance}' if args.use_variance_threshold else f'{args.model}_eval_pca_{args.pca}_components_{args.pca_components}',\n",
    "                                 config=args,\n",
    "                                 )\n",
    "    if args.model == 'graph_sage':\n",
    "        model = Model(hidden_channels=args.hidden_channels, out_channels=args.out_channels, edge_features=1, metadata=train_data.metadata())\n",
    "    elif args.model == 'meta_sage':\n",
    "        model = MetaSage(train_data['customer'].num_nodes, hidden_channels=args.hidden_channels, out_channels=args.out_channels)\n",
    "    elif args.model == 'meta_gatv2':\n",
    "        model = MetaGATv2(train_data['customer'].num_nodes, hidden_channels=args.hidden_channels, out_channels=args.out_channels, edge_channels=args.edge_channels)\n",
    "    elif args.model == 'graph_transformer':\n",
    "        model = MetaTransformerGat(train_data['customer'].num_nodes, hidden_channels=args.hidden_channels, out_channels=args.out_channels)\n",
    "    model.load_state_dict(torch.load(best_model_path))\n",
    "    model.to(args.device)\n",
    "    # test_rmse = test(model, test_loader)\n",
    "    test_rmse = old_test(model, test_data.to(args.device))\n",
    "    if args.track_run:\n",
    "        wb_run_eval.log({'test_rmse': test_rmse})\n",
    "        wb_run_eval.finish()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    PARSER = argparse.ArgumentParser()\n",
    "    PARSER.add_argument('--use_weighted_loss', action='store_true', default=False,\n",
    "                        help='Whether to use weighted MSE loss.')\n",
    "    PARSER.add_argument('--no_epochs', default=20, type=int)\n",
    "    # Wandb logging options\n",
    "    PARSER.add_argument('-entity', '--entity', type=str, default=\"njmarko\",\n",
    "                        help=\"Name of the team. Multiple projects can exist for the same team.\")\n",
    "    PARSER.add_argument('-project_name', '--project_name', type=str, default=\"graph-recommendation-movielens\",\n",
    "                        help=\"Name of the project. Each experiment in the project will be logged separately\"\n",
    "                             \" as a group\")\n",
    "    PARSER.add_argument('-group', '--group', type=str, default=\"paper\",\n",
    "                        help=\"Name of the experiment group. Each model in the experiment group will be logged \"\n",
    "                             \"separately under a different type.\")\n",
    "    PARSER.add_argument('-save_model_wandb', '--save_model_wandb', type=bool, default=True,\n",
    "                        help=\"Save best model to wandb run.\")\n",
    "    PARSER.add_argument('-job_type', '--job_type', type=str, default=\"train\",\n",
    "                        help=\"Job type {train, eval}.\")\n",
    "    PARSER.add_argument('-tags', '--tags', nargs=\"*\", type=str, default=\"train\",\n",
    "                        help=\"Add a list of tags that describe the run.\")\n",
    "    # Model options\n",
    "    model_choices = ['graph_sage', 'meta_sage', 'meta_gatv2', 'graph_transformer']\n",
    "\n",
    "    PARSER.add_argument('-m', '--model', type=str.lower, default=\"meta_sage\",\n",
    "                        choices=model_choices,\n",
    "                        help=f\"Model to be used for training {model_choices}\")\n",
    "    PARSER.add_argument('--hidden_channels', default=64, type=int)\n",
    "    PARSER.add_argument('--out_channels', default=64, type=int)\n",
    "    PARSER.add_argument('--edge_channels', default=5, type=int)\n",
    "    # Training options\n",
    "    PARSER.add_argument('-device', '--device', type=str, default='cuda', help=\"Device to be used\")\n",
    "    PARSER.add_argument('--val_split', default=0.15, type=float)\n",
    "    PARSER.add_argument('--test_split', default=0.15, type=float)\n",
    "    PARSER.add_argument('--word_embeddings', action='store_true', default=True, help='Use movie synopsis word embeddings')\n",
    "\n",
    "    # Optimizer and scheduler options\n",
    "    PARSER.add_argument('--lr', default=3e-4)\n",
    "    PARSER.add_argument('--weight_decay', default=0.05)\n",
    "    PARSER.add_argument('--base_lr', default=5e-3, type=float)\n",
    "    PARSER.add_argument('--max_lr', default=5e-2, type=float)\n",
    "    PARSER.add_argument('-sch_m', '--scheduler_mode', type=str, default=\"triangular2\",\n",
    "                    choices=['triangular', 'triangular2', 'exp_range'],\n",
    "                    help=f\"CyclicLR scheduler: mode {['triangular', 'triangular2', 'exp_range']}\")\n",
    "    PARSER.add_argument('-step_size_up', '--step_size_up', type=int, default=0,\n",
    "                        help=\"CycleLR scheduler: step size up. If 0, then it is automatically calculated.\")\n",
    "\n",
    "    PARSER.add_argument('--track_run', action='store_true', default=True, help='Track run on wandb')\n",
    "\n",
    "    # Batch options\n",
    "    PARSER.add_argument('--batch_size', default=128)\n",
    "    PARSER.add_argument('--num_partitions', default=150)\n",
    "\n",
    "    # Standardization\n",
    "    PARSER.add_argument('--standardize_product_features', action='store_true', default=True, help='Standardize product features')\n",
    "    PARSER.add_argument('--standardize_edge_features', action='store_true', default=True, help='Standardize edge features')\n",
    "\n",
    "    # PCA options\n",
    "    PARSER.add_argument('--pca', type=str.lower, default=\"separated\",\n",
    "                        choices=[\"none\", \"features\", \"embeddings\", \"combined\", \"separated\"],\n",
    "                        help=\"PCA options for feature reduction\")\n",
    "    PARSER.add_argument('--pca_components', type=int, default=32,\n",
    "                        help=\"Number of PCA components to keep\")\n",
    "    PARSER.add_argument('--use_variance_threshold', action='store_true', default=True, help='Use variance threshold for PCA')\n",
    "    PARSER.add_argument('--variance', type=float, default=0.85, help='Variance threshold for PCA')\n",
    "    PARSER.add_argument('--plot_variance', action='store_true', default=False, help='Plot variance ratio')\n",
    "    \n",
    "\n",
    "    ARGS = PARSER.parse_args()\n",
    "    main(ARGS)\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
